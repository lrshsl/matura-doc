
@misc{su_marvel_2023,
	title = {{MARVEL}: Raster Manga Vectorization via Primitive-wise Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/2110.04830},
	shorttitle = {{MARVEL}},
	abstract = {Manga is a fashionable Japanese-style comic form that is composed of black-and-white strokes and is generally displayed as raster images on digital devices. Typical mangas have simple textures, wide lines, and few color gradients, which are vectorizable natures to enjoy the merits of vector graphics, e.g., adaptive resolutions and small file sizes. In this paper, we propose {MARVEL} ({MAnga}'s Raster to {VEctor} Learning), a primitive-wise approach for vectorizing raster mangas by Deep Reinforcement Learning ({DRL}). Unlike previous learning-based methods which predict vector parameters for an entire image, {MARVEL} introduces a new perspective that regards an entire manga as a collection of basic primitives{\textbackslash}textemdash stroke lines, and designs a {DRL} model to decompose the target image into a primitive sequence for achieving accurate vectorization. To improve vectorization accuracies and decrease file sizes, we further propose a stroke accuracy reward to predict accurate stroke lines, and a pruning mechanism to avoid generating erroneous and repeated strokes. Extensive subjective and objective experiments show that our {MARVEL} can generate impressive results and reaches the state-of-the-art level. Our code is open-source at: https://github.com/{SwordHolderSH}/Mang2Vec.},
	number = {{arXiv}:2110.04830},
	publisher = {{arXiv}},
	author = {Su, Hao and Niu, Jianwei and Liu, Xuefeng and Cui, Jiahe and Wan, Ji},
	urldate = {2023-09-06},
	date = {2023-07-18},
	eprinttype = {arxiv},
	eprint = {2110.04830 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/lrs/Zotero/storage/7SD5Z5MZ/2110.html:text/html;Full Text PDF:/home/lrs/Zotero/storage/QZJSSV83/Su et al. - 2023 - MARVEL Raster Manga Vectorization via Primitive-w.pdf:application/pdf},
}

@misc{dziuba_image_2023,
	title = {Image Vectorization: a Review},
	url = {http://arxiv.org/abs/2306.06441},
	doi = {10.48550/arXiv.2306.06441},
	shorttitle = {Image Vectorization},
	abstract = {Nowadays, there are many diffusion and autoregressive models that show impressive results for generating images from text and other input domains. However, these methods are not intended for ultra-high-resolution image synthesis. Vector graphics are devoid of this disadvantage, so the generation of images in this format looks very promising. Instead of generating vector images directly, you can first synthesize a raster image and then apply vectorization. Vectorization is the process of converting a raster image into a similar vector image using primitive shapes. Besides being similar, generated vector image is also required to contain the minimum number of shapes for rendering. In this paper, we focus specifically on machine learning-compatible vectorization methods. We are considering Mang2Vec, Deep Vectorization of Technical Drawings, {DiffVG}, and {LIVE} models. We also provide a brief overview of existing online methods. We also recall other algorithmic methods, Im2Vec and {ClipGEN} models, but they do not participate in the comparison, since there is no open implementation of these methods or their official implementations do not work correctly. Our research shows that despite the ability to directly specify the number and type of shapes, existing machine learning methods work for a very long time and do not accurately recreate the original image. We believe that there is no fast universal automatic approach and human control is required for every method.},
	number = {{arXiv}:2306.06441},
	publisher = {{arXiv}},
	author = {Dziuba, Maria and Jarsky, Ivan and Efimova, Valeria and Filchenkov, Andrey},
	urldate = {2023-09-28},
	date = {2023-06-10},
	eprinttype = {arxiv},
	eprint = {2306.06441 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/home/lrs/Zotero/storage/SXQJDIXD/Dziuba et al. - 2023 - Image Vectorization a Review.pdf:application/pdf;arXiv.org Snapshot:/home/lrs/Zotero/storage/EGTFJB86/2306.html:text/html},
}

@misc{ledig_photo-realistic_2017,
	title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
	url = {http://arxiv.org/abs/1609.04802},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present {SRGAN}, a generative adversarial network ({GAN}) for image super-resolution ({SR}). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score ({MOS}) test shows hugely significant gains in perceptual quality using {SRGAN}. The {MOS} scores obtained with {SRGAN} are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	number = {{arXiv}:1609.04802},
	publisher = {{arXiv}},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	urldate = {2023-11-03},
	date = {2017-05-25},
	eprinttype = {arxiv},
	eprint = {1609.04802 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/lrs/Zotero/storage/SGM7FF6Y/1609.html:text/html;Full Text PDF:/home/lrs/Zotero/storage/8QVLQIJY/Ledig et al. - 2017 - Photo-Realistic Single Image Super-Resolution Usin.pdf:application/pdf},
}

@misc{isola_image--image_2018,
	title = {Image-to-Image Translation with Conditional Adversarial Networks},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	number = {{arXiv}:1611.07004},
	publisher = {{arXiv}},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	urldate = {2023-11-03},
	date = {2018-11-26},
	eprinttype = {arxiv},
	eprint = {1611.07004 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/lrs/Zotero/storage/GANWT737/1611.html:text/html;Full Text PDF:/home/lrs/Zotero/storage/KI2AFJ7G/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf},
}
